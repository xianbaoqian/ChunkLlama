{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c07ffbd-5e61-4a39-b5d3-a065905f8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q auto-gptq optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978d8d66-1c14-4d8a-bc2e-b74c6653d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_path = 'meta-llama/llama-2-7b-chat-hf'\n",
    "LLAMA3_path = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28447736-b491-4959-925b-198e873bcbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "import torch\n",
    "\n",
    "def load_model(path, tokenizer=None):\n",
    "    quantization_config = transformers.GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(path, attn_implementation=\"flash_attention_2\",\n",
    "                                                 device_map=\"auto\", quantization_config=quantization_config, trust_remote_code=True)\n",
    "    #model = transformers.AutoModelForCausalLM.from_pretrained(path, attn_implementation=\"flash_attention_2\", device_map=\"auto\",\n",
    "    #                                            trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2de9aff-7d83-40a8-8538-d468302cc106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "def generate_prompt_landmark(n_garbage, seed, percent):\n",
    "    \"\"\"Generates a text file and inserts an passkey at a random position.\"\"\"\n",
    "    rnd_state = random.get_state()\n",
    "    random.seed(seed)\n",
    "    # n_garbage_prefix = random.randint(0, n_garbage)\n",
    "    n_garbage_prefix = int(percent * n_garbage)\n",
    "    n_garbage_suffix = n_garbage - n_garbage_prefix\n",
    "\n",
    "    task_description = \"There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\"\n",
    "    garbage = \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\"\n",
    "    garbage_inf = \" \".join([garbage] * 50000)\n",
    "    assert len(garbage_inf) >= n_garbage\n",
    "    garbage_prefix = garbage_inf[:n_garbage_prefix]\n",
    "    garbage_suffix = garbage_inf[:n_garbage_suffix]\n",
    "    pass_key = random.randint(50000, 500000)\n",
    "\n",
    "    information_line = f\"The pass key is {pass_key}. Remember it. {pass_key} is the pass key.\"\n",
    "    final_question = \"What is the pass key? The pass key is\"\n",
    "    lines = [\n",
    "        task_description,\n",
    "        garbage_prefix,\n",
    "        information_line,\n",
    "        garbage_suffix,\n",
    "        final_question,\n",
    "    ]\n",
    "    random.set_state(rnd_state)\n",
    "    return \"\\n\".join(lines), str(pass_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec6bd90-167f-4c7c-a6ed-da7c5aca2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205f96a9-150a-4dab-851e-a7cb527a0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, prompt):\n",
    "    return tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a787fb9a-7ebb-4ca2-9f94-540314dd8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokens, max_tokens):\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=tokens.to(model.device), max_new_tokens=max_tokens, num_beams=1, use_cache=False\n",
    "    )\n",
    "    return generation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67051eeb-1411-463f-b96d-5b20a9c9600b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09af19f-1be6-4458-9299-ec919519ff82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a65dadac-71f9-4b9d-abfd-cdb0cf97f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71526976-dc51-4ab5-9232-a7dc8a511a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunkllama_attn_replace import replace_with_chunkmistral, replace_with_chunkllama\n",
    "replace_with_chunkllama(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa8949-9157-4ebd-8db1-3e116e4df608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8167cdd9dc1f46e0a13d6e97a6e84d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42314ce0b8c48638c4661f4c0e9d92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/41.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598d7caf1dc244fbb948ac43dc18beb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163e1a8a523f49a7bd72aec61137eae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a70effb39e42e387e77fedd8baaa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1bb1a0122d4893ab907fcfd3ea8f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the full scirpt on LLAMA3\n",
    "tokenizer3 = transformers.AutoTokenizer.from_pretrained(LLAMA3_path)\n",
    "model3 = load_model(LLAMA3_path, tokenizer3)\n",
    "\n",
    "prompt, answer = generate_prompt_landmark(1000 * 150, 999, 0.05)\n",
    "\n",
    "tokens3 = tokenize(tokenizer3, prompt)\n",
    "ans3 = tokenize(tokenizer3, answer)\n",
    "\n",
    "output3 = generate(model3, tokens3, ans3.shape[-1])\n",
    "decoded3 = tokenizer3.decode(output3[0][1:])\n",
    "\n",
    "# print(prompt, answer)\n",
    "# print(decoded3, answer)\n",
    "\n",
    "# print(decoded3[:-100], answer[:-100])\n",
    "\n",
    "model_answer3 = output3[0, -ans3.shape[-1]:].cpu()\n",
    "ans_in_text = tokenizer3.decode(model_answer3)\n",
    "ans_in_text = ans_in_text.replace('.', '').strip()\n",
    "print(ans_in_text, answer, ans_in_text == answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb8136-eb3d-4750-b80a-3ec32e3db66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1e096-bad1-408e-a052-3213a0155dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c3fbc-6f31-4ff0-a9d1-d01889d5e4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ff534-0bb6-408e-a776-332571b1a466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fc8ec-a39c-485c-b0ec-769222a840bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
